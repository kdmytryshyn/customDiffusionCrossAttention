{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/diffusers/main/en/conceptual/evaluation\n",
    "!pip show diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP score measures the compatibility of image-caption pairs. Higher CLIP scores imply higher compatibility üîº. The CLIP score is a quantitative measurement of the qualitative concept ‚Äúcompatibility‚Äù. Image-caption pair compatibility can also be thought of as the semantic similarity between the image and the caption. CLIP score was found to have high correlation with human judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate clip score\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "\n",
    "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "\n",
    "def calculate_clip_score(images, prompts):\n",
    "    images_int = (images * 255).astype(\"uint8\")\n",
    "    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n",
    "    return round(float(clip_score), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_ckpt = \"RadwaH/DreamBoothAgnes2\"\n",
    "dream_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to(\"cuda\")\n",
    "prompts = [\"close up portrait of sks girl as Christmas elf ,in a film still of jim henson's labyrinth, with christmas elves, full face details, cinematic lighting, hyper realistic facial features, ultra detailed, canon eos 5d, 100mm f/1.8, ISO100\"]\n",
    "dreambooth_images = dream_pipeline(prompts, num_inference_steps=250, guidance_scale=7.5,  num_images_per_prompt=1, output_type=\"numpy\").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt_1_5 = \"runwayml/stable-diffusion-v1-5\"\n",
    "sd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype = torch.float16).to(\"cuda\")\n",
    "images_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, output_type=\"numpy\").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "model_cd = \"RadwaH/CustomDiffusionAgnes2\"\n",
    "pipe = DiffusionPipeline.from_pretrained(model_cd, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.unet.load_attn_procs(\"path-to-save-model\", weight_name=\"pytorch_custom_diffusion_weights.bin\")\n",
    "pipe.load_textual_inversion(\"path-to-save-model\", weight_name=\"<new1>.bin\")\n",
    "\n",
    "cd_image = pipe(prompts, num_inference_steps=100, guidance_scale=6.0, eta=1.0).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreambooth_clip_score = calculate_clip_score(dreambooth_images, prompts)\n",
    "print(f\"Dreambooth CLIP score: {dreambooth_clip_score}\")\n",
    "\n",
    "sd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)\n",
    "print(f\"Stable Diffusin v-1-5 CLIP Score: {sd_clip_score_1_5}\")\n",
    "\n",
    "sd_clip_score_1_5 = calculate_clip_score(cd_image, prompts)\n",
    "print(f\"Stable Diffusin v-1-5 CLIP Score: {sd_clip_score_1_5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Conditioned Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class-conditioned generative models are usually pre-trained on a class-labeled dataset such as ImageNet-1k.\n",
    "Fr√©chet Inception Distance is a measure of similarity between two datasets of images. It was shown to correlate well with the human judgment of visual quality and is most often used to evaluate the quality of samples of Generative Adversarial Networks. FID is calculated by computing the Fr√©chet distance between two Gaussians fitted to feature representations of the Inception network.\n",
    "These two datasets are essentially the dataset of real images and the dataset of fake images (generated images in our case). FID is usually calculated with two large datasets. However, for this document, we will work with two mini datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "\n",
    "def download(url, local_filepath):\n",
    "    r = requests.get(url)\n",
    "    with open(local_filepath, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return local_filepath\n",
    "\n",
    "\n",
    "dummy_dataset_url = \"https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/sample-imagenet-images.zip\"\n",
    "local_filepath = download(dummy_dataset_url, dummy_dataset_url.split(\"/\")[-1])\n",
    "\n",
    "with ZipFile(local_filepath, \"r\") as zipper:\n",
    "    zipper.extractall(\".\")\n",
    "    \n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = \"sample-imagenet-images\"\n",
    "image_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])\n",
    "\n",
    "real_images_ = [np.array(Image.open(path).convert(\"RGB\")) for path in image_paths]\n",
    "# apply some lightweight pre-processing on them to use them for FID calculation\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = torch.tensor(image).unsqueeze(0)\n",
    "    image = image.permute(0, 3, 1, 2) / 255.0\n",
    "    return F.center_crop(image, (256, 256))\n",
    "\n",
    "\n",
    "real_images_ = torch.cat([preprocess_image(image) for image in real_images_])\n",
    "print(real_images_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_path = \"./examples/dreambooth/girl_sks/\"\n",
    "image_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])\n",
    "\n",
    "real_images = [np.array(Image.open(path).convert(\"RGB\")) for path in image_paths]\n",
    "print(real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply some lightweight pre-processing on them to use them for FID calculation\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = torch.tensor(image).unsqueeze(0)\n",
    "    image = image.permute(0, 3, 1, 2) / 255.0\n",
    "    print(image)\n",
    "    image = F.center_crop(image, (256, 256))\n",
    "    print(image)\n",
    "    return \n",
    "\n",
    "\n",
    "real_images = torch.cat([preprocess_image(image) for image in real_images])\n",
    "# torch.Size([10, 3, 256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Images From Finnetuned Dreambooth\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_ckpt = \"RadwaH/DreamBoothAgnes2\"\n",
    "dream_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to(\"cuda\")\n",
    "prompts = [\"a hopeful pretty sks girl, HD\"]\n",
    "dreambooth_images = dream_pipeline(prompts, num_inference_steps=250, guidance_scale=7.5,  num_images_per_prompt=10, output_type=\"numpy\").images\n",
    "\n",
    "dreambooth_images = torch.tensor(dreambooth_images)\n",
    "dreambooth_images = dreambooth_images.permute(0, 3, 1, 2)\n",
    "print(dreambooth_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = FrechetInceptionDistance(feature=64)\n",
    "fid.update(real_images, real=True)\n",
    "fid.update(fake_images, real=False)\n",
    "\n",
    "print(f\"FID: {float(fid.compute())}\")\n",
    "# FID: 177.7147216796875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
